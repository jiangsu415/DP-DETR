from collections import OrderedDict
from functools import partial
from typing import Optional, Union
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange
from timm.models import register_model
from timm.layers import DropPath, to_2tuple, trunc_normal_
from timm.models.vision_transformer import _cfg
from typing import Tuple
from torch import Tensor

class DWConv(nn.Module):
    def __init__(self, dim):
        super(DWConv, self).__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)

    def forward(self, x):
        x = x.permute(0, 3, 1, 2)
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1)
        return x

class KVGather(nn.Module): # ç”¨äºæ ¹æ®è·¯ç”±ç´¢å¼•å’Œæƒé‡ä»é”®å€¼å¯¹å¼ é‡ä¸­é€‰æ‹©ç‰¹å®šçš„é”®å€¼å¯¹
    def __init__(self, mul_weight='none'):
        super().__init__()
        assert mul_weight in ['none', 'soft', 'hard'] # æ–­è¨€æƒé‡ç›¸ä¹˜çš„æ–¹å¼å¿…é¡»åœ¨ ['none', 'soft', 'hard'] ä¸­ï¼Œç¡®ä¿è¾“å…¥çš„æ–¹å¼æ˜¯åˆæ³•çš„
        self.mul_weight = mul_weight

    def forward(self, r_idx:Tensor, r_weight:Tensor, kv:Tensor): # æ¥å—è·¯ç”±ç´¢å¼• r_idxã€è·¯ç”±æƒé‡ r_weight å’Œé”®å€¼å¯¹å¼ é‡ kv ä½œä¸ºè¾“å…¥
        """
        r_idx: (n, p^2, topk) tensor
        r_weight: (n, p^2, topk) tensor
        kv: (n, p^2, w^2, c_kq+c_v)

        Return:
            (n, p^2, topk, w^2, c_kq+c_v) tensor
        """
        # select kv according to routing index
        n, p2, w2, c_kv = kv.size()
        topk = r_idx.size(-1) # è·å–è·¯ç”±ç´¢å¼•ä¸­çš„ Top-k æ•°é‡
        # print(r_idx.size(), r_weight.size())
        # FIXME: gather consumes much memory (topk times redundancy), write cuda kernel?

        topk_kv = torch.gather(kv.view(n, 1, p2, w2, c_kv).expand(-1, p2, -1, -1, -1), # (n, p^2, p^2, w^2, c_kv) without mem cpy kv.view(n, 1, p2, w2, c_kv)ï¼šå°†é”®å€¼å¯¹å¼ é‡ kv é‡æ–°å½¢çŠ¶ä¸º (n, 1, p2, w2, c_kv)ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¼ é‡çš„ç»´åº¦è¢«é‡æ–°æ’åˆ—ä»¥åŒ¹é… torch.gather å‡½æ•°çš„éœ€æ±‚
                               dim=2, # é€šè¿‡è°ƒç”¨ expand æ–¹æ³•ï¼Œå°†ç¬¬äºŒç»´åº¦æ‰©å±•ä¸º p2ï¼Œå³å°†ç»´åº¦ä¸º 1 çš„ç»´åº¦å¤åˆ¶ p2 æ¬¡ï¼Œå…¶ä»–ç»´åº¦ä¿æŒä¸å˜
                               index=r_idx.view(n, p2, topk, 1, 1).expand(-1, -1, -1, w2, c_kv) # (n, p^2, k, w^2, c_kv) å°†è·¯ç”±ç´¢å¼•å¼ é‡ r_idx é‡æ–°å½¢çŠ¶ä¸º (n, p2, topk, 1, 1)ï¼Œä»¥åŒ¹é… torch.gather å‡½æ•°çš„ç´¢å¼•éœ€æ±‚
                               )# é€šè¿‡è°ƒç”¨ expand æ–¹æ³•ï¼Œå°†ç´¢å¼•å¼ é‡çš„æœ€åä¸¤ä¸ªç»´åº¦åˆ†åˆ«æ‰©å±•ä¸º w2 å’Œ c_kvï¼Œä»¥åŒ¹é…é”®å€¼å¯¹å¼ é‡çš„å½¢çŠ¶
        if self.mul_weight == 'soft': # å¦‚æœæƒé‡ç›¸ä¹˜çš„æ–¹å¼ä¸º 'soft'ï¼Œåˆ™å°†æƒé‡ä¹˜ä»¥é€‰æ‹©çš„é”®å€¼å¯¹å¼ é‡
            topk_kv = r_weight.view(n, p2, topk, 1, 1) * topk_kv # (n, p^2, k, w^2, c_kv)
        elif self.mul_weight == 'hard': # å¦‚æœæƒé‡ç›¸ä¹˜çš„æ–¹å¼ä¸º 'hard'ï¼ŒæŠ›å‡ºæœªå®ç°çš„å¼‚å¸¸ï¼Œè¡¨ç¤ºç¡¬è·¯ç”±æ–¹å¼å°šæœªå®ç°
            raise NotImplementedError('differentiable hard routing TBA')
        return topk_kv

class QKVLinear(nn.Module):
    def __init__(self, dim, qk_dim, bias=True): # è¾“å…¥ç»´åº¦ dimã€æŸ¥è¯¢é”®ç»´åº¦ qk_dim å’Œæ˜¯å¦åŒ…å«åç½®é¡¹ bias ä½œä¸ºå‚æ•°
        super().__init__()
        self.dim = dim  # å°†è¾“å…¥ç»´åº¦å’ŒæŸ¥è¯¢é”®ç»´åº¦åˆ†åˆ«ä¿å­˜ä¸ºç±»çš„å±æ€§
        self.qk_dim = qk_dim
        self.qkv = nn.Linear(dim, qk_dim + qk_dim + dim, bias=bias) # åˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚ qkvï¼Œè¾“å…¥ç»´åº¦ä¸º dimï¼Œè¾“å‡ºç»´åº¦ä¸º qk_dim + qk_dim + dimï¼Œå³æŸ¥è¯¢ç»´åº¦ã€é”®ç»´åº¦å’Œå€¼ç»´åº¦çš„æ€»å’Œï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦åŒ…å«åç½®é¡¹

    def forward(self, x):
        q, kv = self.qkv(x).split([self.qk_dim, self.qk_dim+self.dim], dim=-1) # å°†è¾“å…¥å¼ é‡ x ç»è¿‡çº¿æ€§æ˜ å°„æ“ä½œå¾—åˆ°çš„ç»“æœè¿›è¡Œåˆ†å‰²ï¼Œåˆ†åˆ«å¾—åˆ°æŸ¥è¯¢ q å’Œé”®å€¼å¯¹ kvï¼Œåˆ†å‰²çš„ä½ç½®æ ¹æ®æŸ¥è¯¢é”®ç»´åº¦å’Œå€¼ç»´åº¦è¿›è¡Œåˆ’åˆ†
        return q, kv

class TopkRouting(nn.Module): # ç”¨äºæ‰§è¡Œå¯å¾®åˆ†çš„ Top-k è·¯ç”±æ“ä½œ
    """
    differentiable topk routing with scaling
    Args:
        qk_dim: int, feature dimension of query and key #mg
        topk: int, the 'topk'
        qk_scale: int or None, temperature (multiply) of softmax activation
        with_param: bool, wether inorporate learnable params in routing unit
        diff_routing: bool, wether make routing differentiable
        soft_routing: bool, wether make output value multiplied by routing weights
    """
    def __init__(self,
                 qk_dim, # æ¥å—æŸ¥è¯¢å’Œé”®çš„ç‰¹å¾ç»´åº¦ qk_dim
                 topk=4, # Top-k å€¼ topk
                 qk_scale=None, # æ¸©åº¦å‚æ•° qk_scale
                 param_routing=False, # æ˜¯å¦åŒ…å«å­¦ä¹ å‚æ•°çš„è·¯ç”±å•å…ƒ
                 diff_routing=False): # æ˜¯å¦è¿›è¡Œå¯å¾®åˆ†è·¯ç”± diff_routing ä½œä¸ºå‚æ•°
        super().__init__()
        self.topk = topk
        self.qk_dim = qk_dim
        self.scale = qk_scale or qk_dim ** -0.5
        self.diff_routing = diff_routing
        # TODO: norm layer before/after linear?
        self.emb = nn.Linear(qk_dim, qk_dim) if param_routing else nn.Identity() # æ ¹æ®æ˜¯å¦åŒ…å«å­¦ä¹ å‚æ•°çš„è·¯ç”±å•å…ƒï¼Œåˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚æˆ–è€…æ’ç­‰æ˜ å°„
        # routing activation
        self.routing_act = nn.Softmax(dim=-1) # åˆ›å»ºä¸€ä¸ª Softmax æ¿€æ´»å‡½æ•°ï¼Œç”¨äºè®¡ç®—è·¯ç”±æ¦‚ç‡

    def forward(self, query, key):
        if not self.diff_routing: # å¦‚æœä¸è¿›è¡Œå¯å¾®åˆ†è·¯ç”±ï¼Œåˆ™å¯¹æŸ¥è¯¢å’Œé”®è¿›è¡Œ detach() æ“ä½œï¼Œå³æ–­å¼€æ¢¯åº¦ä¼ æ’­
            query, key = query.detach(), key.detach()
        query_hat, key_hat = self.emb(query), self.emb(key) # per-window pooling -> (n, p^2, c) é€šè¿‡çº¿æ€§æ˜ å°„å±‚æˆ–è€…æ’ç­‰æ˜ å°„ï¼Œå¾—åˆ°æŸ¥è¯¢å’Œé”®çš„æ˜ å°„ç»“æœ
        attn_logit = (query_hat*self.scale) @ key_hat.transpose(-2, -1) # (n, p^2, p^2) è®¡ç®—æ³¨æ„åŠ›logitï¼Œå³æŸ¥è¯¢å’Œé”®çš„ç‚¹ç§¯ï¼Œç»è¿‡ç¼©æ”¾åï¼Œå¾—åˆ°æ³¨æ„åŠ›logit
        topk_attn_logit, topk_index = torch.topk(attn_logit, k=self.topk, dim=-1) # (n, p^2, k), (n, p^2, k) æ ¹æ® Top-k å€¼ï¼Œé€‰æ‹©æ³¨æ„åŠ›logit ä¸­çš„å‰ k ä¸ªå€¼ï¼Œå¹¶è¿”å›å¯¹åº”çš„ç´¢å¼•ã€‚
        r_weight = self.routing_act(topk_attn_logit) # (n, p^2, k) é€šè¿‡ Softmax æ¿€æ´»å‡½æ•°è®¡ç®—è·¯ç”±æƒé‡ï¼Œå¾—åˆ°è·¯ç”±æƒé‡
        return r_weight, topk_index # è¿”å›è·¯ç”±æƒé‡ r_weight å’Œ Top-k ç´¢å¼• topk_index

class BiLevelRoutingAttention(nn.Module):
    def __init__(self, dim, # è¾“å…¥ç‰¹å¾çš„ç»´åº¦
                 num_heads=8,
                 n_win=7, # çª—å£çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 7
                 qk_dim=None, # æŸ¥è¯¢å’Œé”®çš„ç»´åº¦ï¼Œå¦‚æœæœªæŒ‡å®šï¼Œåˆ™é»˜è®¤ä¸º Noneï¼Œåœ¨åç»­ä¼šæ ¹æ®æƒ…å†µè®¾ç½®ä¸ºè¾“å…¥ç‰¹å¾çš„ç»´åº¦ dim
                 qk_scale=None, # æŸ¥è¯¢å’Œé”®çš„ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º None
                 kv_per_win=4, # æ¯ä¸ªçª—å£ä¸­é”®å€¼å¯¹çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 4
                 kv_downsample_ratio=4, #é”®å€¼å¯¹ä¸‹é‡‡æ ·çš„æ¯”ç‡ï¼Œé»˜è®¤ä¸º 4
                 kv_downsample_kernel=None,# é”®å€¼å¯¹ä¸‹é‡‡æ ·çš„å·ç§¯æ ¸ï¼Œé»˜è®¤ä¸º None
                 kv_downsample_mode='identity', # é”®å€¼å¯¹ä¸‹é‡‡æ ·çš„æ¨¡å¼ï¼Œé»˜è®¤ä¸º 'identity'ï¼Œè¡¨ç¤ºä¸è¿›è¡Œä¸‹é‡‡æ ·ã€‚
                 topk=4, param_attention="qkvo", # å‚æ•°æ³¨æ„åŠ›çš„è®¾ç½®ï¼Œé»˜è®¤ä¸º "qkvo"ï¼Œè¡¨ç¤ºåŒæ—¶ä½¿ç”¨æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºçš„çº¿æ€§æ˜ å°„
                 param_routing=False,# æ˜¯å¦ä½¿ç”¨å‚æ•°è·¯ç”±ï¼Œé»˜è®¤ä¸º False
                 diff_routing=False, # æ˜¯å¦ä½¿ç”¨ä¸åŒçš„è·¯ç”±æ–¹å¼ï¼Œé»˜è®¤ä¸º False
                 soft_routing=False, # æ˜¯å¦ä½¿ç”¨è½¯è·¯ç”±ï¼Œé»˜è®¤ä¸º False
                 side_dwconv=3, # ä¾§è¾¹æ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„å°ºå¯¸ï¼Œé»˜è®¤ä¸º 3
                 auto_pad=True): # æ˜¯å¦è‡ªåŠ¨å¡«å……ï¼Œé»˜è®¤ä¸º True
        super().__init__()
        # local attention setting
        self.dim = dim
        self.n_win = n_win  # Wh, Ww
        self.num_heads = num_heads
        self.qk_dim = qk_dim or dim
        assert self.qk_dim % num_heads == 0 and self.dim % num_heads==0, 'qk_dim and dim must be divisible by num_heads!'
        self.scale = qk_scale or self.qk_dim ** -0.5
# è¿™ä¸ªå·ç§¯æ“ä½œç”¨äºç”Ÿæˆå±€éƒ¨ä½ç½®åµŒå…¥
        self.lepe = nn.Conv2d(dim, dim, kernel_size=side_dwconv, stride=1, padding=side_dwconv//2, groups=dim) if side_dwconv > 0 else \
            lambda x: torch.zeros_like(x) # è¿™ä¸ªå‡½æ•°ç”¨äºåœ¨ä¸éœ€è¦å·ç§¯æ“ä½œæ—¶è¿”å›ä¸€ä¸ªå…¨é›¶å¼ é‡ï¼Œä¿æŒæ•°æ®çš„å½¢çŠ¶ä¸å˜

        ################ global routing setting ###mg##############
        self.topk = topk
        self.param_routing = param_routing
        self.diff_routing = diff_routing
        self.soft_routing = soft_routing
        # router
        assert not (self.param_routing and not self.diff_routing) # cannot be with_param=True and diff_routing=False # æ–­è¨€ä¸å¯ä»¥åŒæ—¶è®¾ç½®å‚æ•°è·¯ç”±ä¸º True ä¸”ä¸ä½¿ç”¨ä¸åŒçš„è·¯ç”±æ–¹å¼ï¼Œå› ä¸ºå‚æ•°è·¯ç”±éœ€è¦ä½¿ç”¨ä¸åŒçš„è·¯ç”±æ–¹å¼ã€‚
        self.router = TopkRouting(qk_dim=self.qk_dim,
                                  qk_scale=self.scale,
                                  topk=self.topk,
                                  diff_routing=self.diff_routing,
                                  param_routing=self.param_routing) # æ ¹æ®å‚æ•°è®¾ç½®åˆ›å»ºä¸€ä¸ª TopkRouting è·¯ç”±å™¨ï¼Œå…¶ä¸­åŒ…æ‹¬æŸ¥è¯¢é”®ç»´åº¦ã€ç¼©æ”¾å› å­ã€Top-k å€¼ã€ä¸åŒçš„è·¯ç”±æ–¹å¼å’Œå‚æ•°è·¯ç”±çš„è®¾ç½®ã€‚
        if self.soft_routing: # soft routing, always diffrentiable (if no detach) # å¦‚æœè®¾ç½®äº†è½¯è·¯ç”±ï¼Œåˆ™è®¾ç½®æƒé‡ä¹˜æ³•æ–¹å¼ä¸º 'soft'ã€‚
            mul_weight = 'soft'
        elif self.diff_routing: # hard differentiable routing å¦‚æœè®¾ç½®äº†ä¸åŒçš„è·¯ç”±æ–¹å¼ï¼Œåˆ™è®¾ç½®æƒé‡ä¹˜æ³•æ–¹å¼ä¸º 'hard'ï¼Œè¡¨ç¤ºç¡¬å¯å¾®åˆ†è·¯ç”±ã€‚
            mul_weight = 'hard'
        else:  # hard non-differentiable routing å¦‚æœæ²¡æœ‰è®¾ç½®è½¯è·¯ç”±æˆ–ä¸åŒçš„è·¯ç”±æ–¹å¼ï¼Œåˆ™è®¾ç½®æƒé‡ä¹˜æ³•æ–¹å¼ä¸º 'none'ï¼Œè¡¨ç¤ºç¡¬éå¯å¾®åˆ†è·¯ç”±ã€‚
            mul_weight = 'none'
        self.kv_gather = KVGather(mul_weight=mul_weight) # æ ¹æ®æƒé‡ä¹˜æ³•æ–¹å¼åˆ›å»ºä¸€ä¸ª KVGather æ¨¡å—ï¼Œç”¨äºæ”¶é›†é”®å€¼ä¿¡æ¯

        # qkv mapping (shared by both global routing and local attention)
        self.param_attention = param_attention
        if self.param_attention == 'qkvo': # å¦‚æœå‚æ•°è®¾ç½®ä¸º 'qkvo'ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª QKVLinear æ¨¡å—ç”¨äºæŸ¥è¯¢ã€é”®ã€å€¼çº¿æ€§æ˜ å°„ï¼ŒåŒæ—¶åˆ›å»ºä¸€ä¸ªå…¨è¿æ¥å±‚ nn.Linear ç”¨äºæœ€ç»ˆè¾“å‡º
            self.qkv = QKVLinear(self.dim, self.qk_dim)
            self.wo = nn.Linear(dim, dim)
        elif self.param_attention == 'qkv': # å¦‚æœå‚æ•°è®¾ç½®ä¸º 'qkv'ï¼ŒåŒæ ·åˆ›å»ºä¸€ä¸ª QKVLinear æ¨¡å—ç”¨äºæŸ¥è¯¢ã€é”®ã€å€¼çº¿æ€§æ˜ å°„ï¼Œä½†æ˜¯è¾“å‡ºç›´æ¥ä½¿ç”¨ nn.Identity()ï¼Œè¡¨ç¤ºä¸è¿›è¡Œé¢å¤–çš„æ˜ å°„å¤„ç†
            self.qkv = QKVLinear(self.dim, self.qk_dim)
            self.wo = nn.Identity()
        else:
            raise ValueError(f'param_attention mode {self.param_attention} is not surpported!')

        self.kv_downsample_mode = kv_downsample_mode
        self.kv_per_win = kv_per_win
        self.kv_downsample_ratio = kv_downsample_ratio
        self.kv_downsample_kenel = kv_downsample_kernel
        if self.kv_downsample_mode == 'ada_avgpool':
            assert self.kv_per_win is not None
            self.kv_down = nn.AdaptiveAvgPool2d(self.kv_per_win) # åˆ›å»ºä¸€ä¸ªè‡ªé€‚åº”å¹³å‡æ± åŒ–å±‚ nn.AdaptiveAvgPool2d
        elif self.kv_downsample_mode == 'ada_maxpool':
            assert self.kv_per_win is not None
            self.kv_down = nn.AdaptiveMaxPool2d(self.kv_per_win) # åˆ›å»ºä¸€ä¸ªè‡ªé€‚åº”æœ€å¤§æ± åŒ–å±‚ nn.AdaptiveMaxPool2d
        elif self.kv_downsample_mode == 'maxpool':
            assert self.kv_downsample_ratio is not None # åˆ›å»ºä¸€ä¸ªæœ€å¤§æ± åŒ–å±‚ nn.MaxPool2d æˆ–è€…æ’ç­‰æ˜ å°„ nn.Identity()ï¼Œå–å†³äºä¸‹é‡‡æ ·æ¯”ç‡æ˜¯å¦å¤§äº 1
            self.kv_down = nn.MaxPool2d(self.kv_downsample_ratio) if self.kv_downsample_ratio > 1 else nn.Identity()
        elif self.kv_downsample_mode == 'avgpool':
            assert self.kv_downsample_ratio is not None # åˆ›å»ºä¸€ä¸ªå¹³å‡æ± åŒ–å±‚ nn.AvgPool2d æˆ–è€…æ’ç­‰æ˜ å°„ nn.Identity()ï¼Œå–å†³äºä¸‹é‡‡æ ·æ¯”ç‡æ˜¯å¦å¤§äº 1ã€‚
            self.kv_down = nn.AvgPool2d(self.kv_downsample_ratio) if self.kv_downsample_ratio > 1 else nn.Identity()
        elif self.kv_downsample_mode == 'identity': # no kv downsampling ç›´æ¥åˆ›å»ºä¸€ä¸ªæ’ç­‰æ˜ å°„ nn.Identity()ï¼Œè¡¨ç¤ºä¸è¿›è¡Œé”®å€¼å¯¹ä¸‹é‡‡æ ·æ“ä½œ
            self.kv_down = nn.Identity()
        elif self.kv_downsample_mode == 'fracpool':
            raise NotImplementedError('fracpool policy is not implemented yet!')
        elif kv_downsample_mode == 'conv':
            # TODO: need to consider the case where k != v so that need two downsample modules
            raise NotImplementedError('conv policy is not implemented yet!')
        else:
            raise ValueError(f'kv_down_sample_mode {self.kv_downsaple_mode} is not surpported!')

        # softmax for local attention
        self.attn_act = nn.Softmax(dim=-1) # ç”¨äºåœ¨å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¯¹è¾“å…¥è¿›è¡Œ Softmax æ“ä½œã€‚é€šè¿‡è®¾ç½® dim=-1ï¼Œè¡¨ç¤ºå¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œ Softmax æ“ä½œï¼Œé€šå¸¸ç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡

        self.auto_pad=auto_pad # ä¼ å…¥çš„ auto_pad å‚æ•°èµ‹å€¼ç»™ç±»çš„å±æ€§ auto_padï¼Œè¡¨ç¤ºæ˜¯å¦è‡ªåŠ¨å¡«å……çš„è®¾ç½®ã€‚è¿™ä¸ªå±æ€§å¯èƒ½åœ¨æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ç”¨äºæ§åˆ¶æ˜¯å¦è¿›è¡Œè‡ªåŠ¨å¡«å……æ“ä½œ

    def forward(self, x):
        N, H, W, C = x.size() # è·å–è¾“å…¥å¼ é‡ x çš„å¤§å°ï¼Œå…¶ä¸­ N è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼ŒH å’Œ W è¡¨ç¤ºé«˜åº¦å’Œå®½åº¦ï¼ŒC è¡¨ç¤ºé€šé“æ•°

        # patchify, (n, p^2, w, w, c), keep 2d window as we need 2d pooling to reduce kv size
        x = rearrange(x, "n (j h) (i w) c -> n (j i) h w c", j=self.n_win, i=self.n_win) # å¯¹è¾“å…¥å¼ é‡ x è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œå°†å…¶åˆ’åˆ†ä¸ºå¤šä¸ªå°çª—å£ï¼Œä»¥ä¾¿è¿›è¡Œå±€éƒ¨æ³¨æ„åŠ›è®¡ç®—
        q, kv = self.qkv(x) # é€šè¿‡æŸ¥è¯¢é”®å€¼çº¿æ€§æ˜ å°„è·å–æŸ¥è¯¢ q å’Œé”®å€¼å¯¹ kv
        # pixel-wise qkv
        # q_pix: (n, p^2, w^2, c_qk)
        # kv_pix: (n, p^2, h_kv*w_kv, c_qk+c_v)
        q_pix = rearrange(q, 'n p2 h w c -> n p2 (h w) c') # å¯¹æŸ¥è¯¢å¼ é‡ q è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œå°†å…¶ä» (n, p^2, h, w, c) çš„å½¢çŠ¶è½¬æ¢ä¸º (n, p^2, h*w, c) çš„å½¢çŠ¶ï¼Œå°†æŸ¥è¯¢å¼ é‡æŒ‰ç…§åƒç´ çº§é‡æ–°ç»„ç»‡
        kv_pix = self.kv_down(rearrange(kv, 'n p2 h w c -> (n p2) c h w')) # å¯¹é”®å€¼å¯¹å¼ é‡ kv è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œå¹¶é€šè¿‡ kv_down æ¨¡å—è¿›è¡Œä¸‹é‡‡æ ·æ“ä½œï¼Œå°†å…¶ä» (n, p^2, h, w, c) çš„å½¢çŠ¶è½¬æ¢ä¸º (np^2, c, h, w) çš„å½¢çŠ¶ï¼Œ
        kv_pix = rearrange(kv_pix, '(n j i) c h w -> n (j i) (h w) c', j=self.n_win, i=self.n_win) # ç„¶åå†å°†å…¶é‡æ–°æ’åˆ—ä¸º (n, ji, h*w, c) çš„å½¢çŠ¶ï¼Œå…¶ä¸­ j å’Œ i åˆ†åˆ«ä¸ºçª—å£çš„é«˜åº¦å’Œå®½åº¦
# è®¡ç®—çª—å£çº§çš„æŸ¥è¯¢ q_win å’Œé”® k_winï¼Œåˆ†åˆ«é€šè¿‡å¯¹æŸ¥è¯¢å¼ é‡ q åœ¨ç¬¬ 2 å’Œç¬¬ 3 ç»´åº¦ä¸Šè¿›è¡Œå¹³å‡å€¼è®¡ç®—ï¼Œä»¥åŠå¯¹é”®å€¼å¯¹å¼ é‡ kv åœ¨ç¬¬ 2 å’Œç¬¬ 3 ç»´åº¦ä¸Šè¿›è¡Œå¹³å‡å€¼è®¡ç®—è·å–é”®
        q_win, k_win = q.mean([2, 3]), kv[..., 0:self.qk_dim].mean([2, 3]) # window-wise qk, (n, p^2, c_qk), (n, p^2, c_qk)

        ##################side_dwconv(lepe)##################
        # NOTE: call contiguous to avoid gradient warning when using ddp å¯¹é”®å€¼å¯¹å¼ é‡ä¸­çš„ä¸€éƒ¨åˆ†ï¼ˆä» self.qk_dim å¼€å§‹çš„éƒ¨åˆ†ï¼‰è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œå¹¶é€šè¿‡ä¾§è¾¹æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¨¡å— lepe è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ° lepe å¼ é‡ã€‚ä¸ºäº†é¿å…æ¢¯åº¦è­¦å‘Šï¼Œä½¿ç”¨ contiguous() æ–¹æ³•
        lepe = self.lepe(rearrange(kv[..., self.qk_dim:], 'n (j i) h w c -> n c (j h) (i w)', j=self.n_win, i=self.n_win).contiguous())
        lepe = rearrange(lepe, 'n c (j h) (i w) -> n (j h) (i w) c', j=self.n_win, i=self.n_win) # å°†å¤„ç†åçš„ lepe å¼ é‡é‡æ–°æ’åˆ—ï¼Œå°†å…¶ä» (n, c, jh, iw) çš„å½¢çŠ¶è½¬æ¢ä¸º (n, jh, iw, c) çš„å½¢çŠ¶ï¼Œä»¥ä¾¿åç»­çš„æ“ä½œ

        ############ gather q dependent k/v #################

        r_weight, r_idx = self.router(q_win, k_win) # both are (n, p^2, topk) tensors é€šè¿‡è·¯ç”±å™¨æ¨¡å— router å¯¹æŸ¥è¯¢ q_win å’Œé”® k_win è¿›è¡Œè·¯ç”±æ“ä½œï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡å’Œç´¢å¼•ï¼Œä¸¤è€…éƒ½æ˜¯å½¢çŠ¶ä¸º (n, p^2, topk) çš„å¼ é‡

        kv_pix_sel = self.kv_gather(r_idx=r_idx, r_weight=r_weight, kv=kv_pix) #(n, p^2, topk, h_kv*w_kv, c_qk+c_v) æ ¹æ®è·¯ç”±å¾—åˆ°çš„ç´¢å¼•å’Œæƒé‡ï¼Œé€šè¿‡é”®å€¼å¯¹é€‰æ‹©æ¨¡å— kv_gather ä»é”®å€¼å¯¹å¼ é‡ kv_pix ä¸­é€‰æ‹©ç‰¹å®šçš„é”®å€¼å¯¹ï¼Œå¾—åˆ°å½¢çŠ¶ä¸º (n, p^2, topk, h_kv*w_kv, c_qk+c_v) çš„å¼ é‡
        k_pix_sel, v_pix_sel = kv_pix_sel.split([self.qk_dim, self.dim], dim=-1) # å°†é€‰æ‹©çš„é”®å€¼å¯¹å¼ é‡æŒ‰ç…§æŸ¥è¯¢é”®å’Œå€¼çš„ç»´åº¦è¿›è¡Œåˆ†å‰²ï¼Œå¾—åˆ°é”®å¼ é‡ k_pix_sel å’Œå€¼å¼ é‡ v_pix_selï¼Œåˆ†åˆ«ä¸ºå½¢çŠ¶ (n, p^2, topk, h_kvw_kv, c_qk) å’Œ (n, p^2, topk, h_kvw_kv, c_v)

        ######### do attention as normal ####################
        k_pix_sel = rearrange(k_pix_sel, 'n p2 k w2 (m c) -> (n p2) m c (k w2)', m=self.num_heads) # flatten to BMLC, (n*p^2, m, topk*h_kv*w_kv, c_kq//m) transpose here? å°†é”®å¼ é‡ k_pix_sel é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (np^2, m, c, kw2)ï¼Œå…¶ä¸­ m è¡¨ç¤ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œå°†å…¶å±•å¹³ä¸º BMLC çš„å½¢å¼ï¼Œä»¥ä¾¿åç»­çš„çŸ©é˜µä¹˜æ³•æ“ä½œ
        v_pix_sel = rearrange(v_pix_sel, 'n p2 k w2 (m c) -> (n p2) m (k w2) c', m=self.num_heads) # flatten to BMLC, (n*p^2, m, topk*h_kv*w_kv, c_v//m) å°†å€¼å¼ é‡ v_pix_sel é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (np^2, m, kw2, c)ï¼ŒåŒæ ·å±•å¹³ä¸º BMLC çš„å½¢å¼
        q_pix = rearrange(q_pix, 'n p2 w2 (m c) -> (n p2) m w2 c', m=self.num_heads) # to BMLC tensor (n*p^2, m, w^2, c_qk//m) å°†æŸ¥è¯¢å¼ é‡ q_pix é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (n*p^2, m, w2, c)ï¼ŒåŒæ ·å±•å¹³ä¸º BMLC çš„å½¢å¼

        # param-free multihead attention
        attn_weight = (q_pix * self.scale) @ k_pix_sel # (n*p^2, m, w^2, c) @ (n*p^2, m, c, topk*h_kv*w_kv) -> (n*p^2, m, w^2, topk*h_kv*w_kv) è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œé¦–å…ˆå¯¹æŸ¥è¯¢å¼ é‡ä¹˜ä»¥ç¼©æ”¾å› å­ï¼Œç„¶åä¸é”®å¼ é‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ“ä½œï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º (np^2, m, w2, kh_kv*w_kv)
        attn_weight = self.attn_act(attn_weight) # é€šè¿‡ Softmax æ¿€æ´»å‡½æ•°å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿æ³¨æ„åŠ›æƒé‡çš„å’Œä¸º1
        out = attn_weight @ v_pix_sel # (n*p^2, m, w^2, topk*h_kv*w_kv) @ (n*p^2, m, topk*h_kv*w_kv, c) -> (n*p^2, m, w^2, c) å°†å½’ä¸€åŒ–åçš„æ³¨æ„åŠ›æƒé‡ä¸å€¼å¼ é‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ“ä½œï¼Œå¾—åˆ°è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (n*p^2, m, w2, c_v)
        out = rearrange(out, '(n j i) m (h w) c -> n (j h) (i w) (m c)', j=self.n_win, i=self.n_win,
                        h=H//self.n_win, w=W//self.n_win) # å°†è¾“å‡ºå¼ é‡é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (n, jh, iw, m*c)ï¼Œå…¶ä¸­ j å’Œ i åˆ†åˆ«è¡¨ç¤ºçª—å£çš„é«˜åº¦å’Œå®½åº¦ï¼Œh å’Œ w åˆ†åˆ«è¡¨ç¤ºè¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦
        out = out + lepe # å°†è¾“å‡ºå¼ é‡ä¸ä¾§è¾¹æ·±åº¦å¯åˆ†ç¦»å·ç§¯å¤„ç†åçš„å¼ é‡ç›¸åŠ ï¼Œç”¨äºå¼•å…¥å±€éƒ¨ä½ç½®ä¿¡æ¯
        # output linear
        out = self.wo(out)
        return out

class BiFormerBlock(nn.Module):
    def __init__(self, dim,
                 outdim,
                 n_win,
                 drop_path=0.,
                 layer_scale_init_value=-1,
                 num_heads=8,
                 qk_dim=None,
                 qk_scale=None,
                 kv_per_win=4,
                 kv_downsample_ratio=4,
                 kv_downsample_kernel=None,
                 kv_downsample_mode='ada_avgpool',
                 topk=4,
                 param_attention="qkvo",
                 param_routing=False,
                 diff_routing=False,
                 soft_routing=False,
                 mlp_ratio=4,
                 mlp_dwconv=False,
                 side_dwconv=5,
                 before_attn_dwconv=3,
                 pre_norm=True,
                 auto_pad=False):
        super().__init__()
        qk_dim = qk_dim or dim # å¦‚æœ qk_dim çš„å€¼ä¸º Noneï¼Œåˆ™å°†å…¶è®¾ä¸º dimã€‚è¿™è¡Œä»£ç ç”¨äºè®¾ç½®æŸ¥è¯¢å’Œé”®çš„ç»´åº¦ï¼Œå¦‚æœæ²¡æœ‰æ˜¾å¼æä¾› qk_dim çš„å€¼ï¼Œå°±ä½¿ç”¨è¾“å…¥ç‰¹å¾çš„ç»´åº¦ dim

        # modules
        if before_attn_dwconv > 0: # å¤§äº 0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªå·ç§¯æ“ä½œ nn.Conv2d ä½œä¸ºä½ç½®åµŒå…¥ pos_embedï¼›å¦åˆ™ï¼Œåˆ›å»ºä¸€ä¸ª lambda å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•° x å¹¶è¿”å› 0ã€‚
            self.pos_embed = nn.Conv2d(dim, dim,  kernel_size=before_attn_dwconv, padding=1, groups=dim)
        else:
            self.pos_embed = lambda x: 0
        self.norm1 = nn.LayerNorm(dim, eps=1e-6) # important to avoid attention collapsing ç”¨äºå¯¹è¾“å…¥æ•°æ®è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œä»¥é¿å…æ³¨æ„åŠ›æŠ˜å ï¼ˆattention collapsingï¼‰
        if topk > 0: # æ ¹æ® topk çš„å€¼è¿›è¡Œæ¡ä»¶åˆ¤æ–­ï¼Œå¦‚æœå¤§äº 0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªBiLevelRoutingAttention æ³¨æ„åŠ›æ¨¡å— attn
            self.attn = BiLevelRoutingAttention(dim=dim, num_heads=num_heads, n_win=n_win, qk_dim=qk_dim,
                                                qk_scale=qk_scale, kv_per_win=kv_per_win, kv_downsample_ratio=kv_downsample_ratio,
                                                kv_downsample_kernel=kv_downsample_kernel, kv_downsample_mode=kv_downsample_mode,
                                                topk=topk, param_attention=param_attention, param_routing=param_routing,
                                                diff_routing=diff_routing, soft_routing=soft_routing, side_dwconv=side_dwconv,
                                                auto_pad=auto_pad)
        elif topk == 0: # å¦‚æœç­‰äº 0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåºåˆ—æ¨¡å— nn.Sequentialï¼Œå…¶ä¸­åŒ…å«ä¸€ç³»åˆ—å·ç§¯æ“ä½œï¼Œç”¨äºæ¨¡æ‹Ÿæ³¨æ„åŠ›æœºåˆ¶
            self.attn = nn.Sequential(Rearrange('n h w c -> n c h w'), # compatiability
                                      nn.Conv2d(dim, dim, 1), # pseudo qkv linear
                                      nn.Conv2d(dim, dim, 5, padding=2, groups=dim), # pseudo attention
                                      nn.Conv2d(dim, dim, 1), # pseudo out linear
                                      Rearrange('n c h w -> n h w c')# ğŸ¥­
                                      )
        self.norm2 = nn.LayerNorm(dim, eps=1e-6) # åˆ›å»ºç¬¬äºŒä¸ª Layer Normalization æ“ä½œ nn.LayerNormï¼Œç”¨äºå¯¹è¾“å‡ºæ•°æ®è¿›è¡Œè§„èŒƒåŒ–å¤„ç†
        self.mlp = nn.Sequential(nn.Linear(dim, int(mlp_ratio*dim)),
                                 DWConv(int(mlp_ratio*dim)) if mlp_dwconv else nn.Identity(),
                                 nn.GELU(),
                                 nn.Linear(int(mlp_ratio*dim), dim)
                                 ) #mg # åˆ›å»ºä¸€ä¸ªåŒ…å«çº¿æ€§å±‚ã€æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€GELU æ¿€æ´»å‡½æ•°å’Œå¦ä¸€ä¸ªçº¿æ€§å±‚çš„åºåˆ—æ¨¡å— mlpï¼Œç”¨äºå®ç°å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰éƒ¨åˆ†
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity() # åˆ™åˆ›å»ºä¸€ä¸ª DropPath æ“ä½œ DropPath,å¦åˆ™ï¼Œåˆ›å»ºä¸€ä¸ªæ’ç­‰æ˜ å°„ nn.Identity()

        # tricks: layer scale & pre_norm/post_norm
        if layer_scale_init_value > 0:
            self.use_layer_scale = True # è®¾ç½®ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨å±‚æ ‡å‡†åŒ–
            self.gamma1 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) # åˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•° gamma1ï¼Œå…¶å€¼ä¸º layer_scale_init_value ä¹˜ä»¥ä¸€ä¸ªç»´åº¦ä¸º dim çš„å…¨ä¸º 1 çš„å¼ é‡ï¼Œè¯¥å‚æ•°å°†ç”¨äºå±‚æ ‡å‡†åŒ–
            self.gamma2 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) # ä¸ gamma1 ç±»ä¼¼ï¼Œç”¨äºå¦ä¸€ç§å±‚æ ‡å‡†åŒ–
        else:
            self.use_layer_scale = False # å¦‚æœ layer_scale_init_value ä¸å¤§äº 0ï¼Œå°† use_layer_scale è®¾ç½®ä¸º Falseï¼Œè¡¨ç¤ºä¸ä½¿ç”¨å±‚æ ‡å‡†åŒ–
        self.pre_norm = pre_norm # è®¾ç½®ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿›è¡Œé¢„æ ‡å‡†åŒ–
        self.outdim = outdim # å°†è¾“å‡ºç»´åº¦ä¿å­˜åœ¨ outdim ä¸­ï¼Œè¿™ä¸ªå€¼åœ¨ä¹‹åå¯èƒ½ä¼šè¢«ç”¨åˆ°


    def forward(self, x):
        # conv pos embedding
        x = x + self.pos_embed(x) # å°†è¾“å…¥ x ä¸ä½ç½®åµŒå…¥ pos_embed ç›¸åŠ ï¼Œç”¨äºå¼•å…¥ä½ç½®ä¿¡æ¯
        # permute to NHWC tensor for attention & mlp #mg
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)å¯¹å¼ é‡ x è¿›è¡Œç»´åº¦ç½®æ¢ï¼Œå°†å…¶ä» (N, C, H, W) çš„æ ¼å¼è½¬æ¢ä¸º (N, H, W, C) çš„æ ¼å¼ï¼Œä»¥ä¾¿åç»­çš„æ³¨æ„åŠ›å’Œå¤šå±‚æ„ŸçŸ¥æœºæ“ä½œ

        # attention & mlp
        if self.pre_norm:
            if self.use_layer_scale: # å¦‚æœè¿›è¡Œé¢„æ ‡å‡†åŒ–å¹¶ä¸”ä½¿ç”¨å±‚æ ‡å‡†åŒ–,åˆ™å¯¹è¾“å…¥è¿›è¡Œè§„èŒƒåŒ–åï¼Œåˆ†åˆ«åº”ç”¨æ³¨æ„åŠ›å’Œå¤šå±‚æ„ŸçŸ¥æœºæ“ä½œï¼Œå¹¶åŠ ä¸Š DropPath æ“ä½œ
                x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x))) # (N, H, W, C)
                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x))) # (N, H, W, C)
            else: # ç›´æ¥åº”ç”¨æ³¨æ„åŠ›å’Œå¤šå±‚æ„ŸçŸ¥æœºæ“ä½œï¼Œå¹¶åŠ ä¸Š DropPath æ“ä½œ
                x = x + self.drop_path(self.attn(self.norm1(x))) # (N, H, W, C)
                x = x + self.drop_path(self.mlp(self.norm2(x))) # (N, H, W, C)
        else:
            if self.use_layer_scale:
                x = self.norm1(x + self.drop_path(self.gamma1 * self.attn(x))) # (N, H, W, C)
                x = self.norm2(x + self.drop_path(self.gamma2 * self.mlp(x))) # (N, H, W, C)
            else:
                x = self.norm1(x + self.drop_path(self.attn(x))) # (N, H, W, C)
                x = self.norm2(x + self.drop_path(self.mlp(x))) # (N, H, W, C)

        # permute back
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)
        return x